ubuntu@gptesting:~/sample$ aws iam create-policy     --policy-name AWSLoadBalancerControllerIAMPolicy     --policy-document file://iam_policy.json

{
    "Policy": {
        "PolicyName": "AWSLoadBalancerControllerIAMPolicy",
        "PolicyId": "ANPAZKRGQYHZ44AIGBEW6",
        "Arn": "arn:aws:iam::641104658931:policy/AWSLoadBalancerControllerIAMPolicy",
        "Path": "/",
        "DefaultVersionId": "v1",
        "AttachmentCount": 0,
        "PermissionsBoundaryUsageCount": 0,
        "IsAttachable": true,
        "CreateDate": "2024-09-18T04:32:25+00:00",
        "UpdateDate": "2024-09-18T04:32:25+00:00"
    }
}
ubuntu@gptesting:~/sample$


ubuntu@gptesting:~/sample$ eksctl create iamserviceaccount \
  --cluster=kubernetes-cluster \
  --namespace=kube-system \
  --name=aws-load-balancer-controller \
  --role-name AmazonEKSLoadBalancerControllerRole \
  --attach-policy-arn=arn:aws:iam::641104658931:policy/AWSLoadBalancerControllerIAMPolicy \
  --approve


eksctl create cluster --name kubernetes-cluster --region ap-south-1 --nodegroup-name standard-workers --node-type t3.medium --nodes 3 --nodes-min 1 --nodes-max 4

if the cluster already exists(Execute the below command)
aws cloudformation delete-stack --region ap-south-1 --stack-name eksctl-k8s-cluster-cluster 

eksctl utils associate-iam-oidc-provider \
  --region=ap-south-1 \
  --cluster=kubernetes-cluster \
  --approve


eksctl create iamserviceaccount \
  --cluster=kubernetes-cluster \
  --namespace=kube-system \
  --name=aws-load-balancer-controller \
  --role-name AmazonEKSLoadBalancerControllerRole \
  --attach-policy-arn=arn:aws:iam::641104658931:policy/AWSLoadBalancerControllerIAMPolicy \
  --approve

kubectl apply \
    --validate=false \
    -f https://github.com/jetstack/cert-manager/releases/download/v1.13.5/cert-manager.yaml

curl -Lo v2_7_2_full.yaml https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.7.2/v2_7_2_full.yaml

sed -i.bak -e '612,620d' ./v2_7_2_full.yaml

docker pull public.ecr.aws/eks/aws-load-balancer-controller:v2.7.2

<replace this with your aws id : 641104658931>

sed -i.bak -e 's|public.ecr.aws/eks/aws-load-balancer-controller|641104658931.dkr.ecr.ap-south-1.amazonaws.com/eks/aws-load-balancer-controller|' ./v2_7_2_full.yaml

kubectl apply -f ./v2_7_2_full.yaml

grep '641104658931.dkr.ecr.ap-south-1.amazonaws.com/eks/aws-load-balancer-controller' ./v2_7_2_full.yaml

kubectl apply -f ./v2_7_2_full.yaml

curl -Lo v2_7_2_ingclass.yaml https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.7.2/v2_7_2_ingclass.yaml

kubectl apply -f v2_7_2_ingclass.yaml

docker images 641104658931.dkr.ecr.ap-south-1.amazonaws.com/eks/aws-load-balancer-controller

aws ecr create-repository --repository-name eks/aws-load-balancer-controller --region ap-south-1

docker tag public.ecr.aws/eks/aws-load-balancer-controller:v2.7.2 641104658931.dkr.ecr.ap-south-1.amazonaws.com/eks/aws-load-balancer-controller:v2.7.2

docker push 641104658931.dkr.ecr.ap-south-1.amazonaws.com/eks/aws-load-balancer-controller:v2.7.2

----------------------------------------------------------------------------------------------------------------------------------------------------------

eksctl get nodegroup --cluster kubernetes-cluster --name standard-workers -o yaml

kubectl apply -f https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.4.7/v2_4_7_full.yaml

aws eks list-clusters --region ap-south-1

kubectl delete clusterrole aws-load-balancer-controller-role

kubectl delete clusterrolebinding aws-load-balancer-controller-rolebinding

kubectl delete role aws-load-balancer-controller-leader-election-role -n kube-system

kubectl delete rolebinding aws-load-balancer-controller-leader-election-rolebinding -n kube-system

kubectl delete service aws-load-balancer-webhook-service -n kube-system

kubectl delete deployment aws-load-balancer-controller -n kube-system

kubectl delete mutatingwebhookconfiguration aws-load-balancer-webhook

kubectl delete validatingwebhookconfiguration aws-load-balancer-webhook

aws iam detach-role-policy --role-name AmazonEKSLoadBalancerControllerRole --policy-arn arn:aws:iam::641104658931:policy/AWSLoadBalancerControllerIAMPolicy

aws iam list-attached-role-policies --role-name AmazonEKSLoadBalancerControllerRole

aws iam delete-policy --policy-arn arn:aws:iam::641104658931:policy/AWSLoadBalancerControllerIAMPolicy


ubuntu@gptesting:~/sample$ aws iam create-policy \
  --policy-name AWSLoadBalancerControllerIAMPolicy \
  --policy-document file://iam_policy.json
{
    "Policy": {
        "PolicyName": "AWSLoadBalancerControllerIAMPolicy",
        "PolicyId": "ANPAZKRGQYHZ5U7VVT473",
        "Arn": "arn:aws:iam::641104658931:policy/AWSLoadBalancerControllerIAMPolicy",
        "Path": "/",
        "DefaultVersionId": "v1",
        "AttachmentCount": 0,
        "PermissionsBoundaryUsageCount": 0,
        "IsAttachable": true,
        "CreateDate": "2024-09-18T09:46:49+00:00",
        "UpdateDate": "2024-09-18T09:46:49+00:00"
    }
}



eksctl create iamserviceaccount \
  --cluster kubernetes-cluster \
  --namespace kube-system \
  --name aws-load-balancer-controller \
  --attach-policy-arn arn:aws:iam::641104658931:policy/AWSLoadBalancerControllerIAMPolicy \
  --approve


aws configure get region

aws ec2 describe-subnets --output table

aws configure set region ap-south-1

aws ec2 describe-subnets --query 'Subnets[*].{ID:SubnetId,Tags:Tags}' --output table

aws ec2 create-tags --resources subnet-03636dd3198024e1b --tags Key=kubernetes.io/role/elb,Value=1

helm uninstall aws-load-balancer-controller -n kube-system

kubectl get pods -n kube-system

kubectl get deployment aws-load-balancer-controller -n kube-system

kubectl get serviceaccount aws-load-balancer-controller -n kube-system -o yaml

aws elbv2 describe-load-balancers

aws ec2 describe-subnets --query "Subnets[?SubnetId=='subnet-03636dd3198024e1b'].Tags"

kubectl logs deployment/aws-load-balancer-controller -n kube-system

eksctl get nodegroup --cluster kubernetes-cluster



----------------------------------------------------------------------------------------------------

eksctl delete iamserviceaccount \
  --cluster kubernetes-cluster \
  --namespace kube-system \
  --name aws-load-balancer-controller

aws iam delete-policy --policy-arn arn:aws:iam::641104658931:policy/AWSLoadBalancerControllerIAMPolicy

aws ec2 delete-tags --resources subnet-03636dd3198024e1b --tags Key=kubernetes.io/role/elb

aws ec2 delete-tags --resources subnet-03636dd3198024e1b --tags Key=kubernetes.io/role/internal-elb

aws cloudformation delete-stack --region ap-south-1 --stack-name eksctl-k8s-cluster-cluster

aws cloudformation delete-stack --region ap-south-1 --stack-name eksctl-k8s-cluster-nodegroup-standard-workers

------------------------------------------------------------------------------------------------------

eksctl create cluster \
  --name k8s-cluster \
  --region ap-south-1 \
  --nodegroup-name standard-workers \
  --node-type t3.medium \
  --nodes 1 \
  --nodes-min 1 \
  --nodes-max 1

----------------------------------------------------------------------------------------------------------------

ubuntu@gptesting:~/sample$ eksctl create cluster   --name kubernetes-cluster   --region ap-south-1   --nodegroup-name standard-workers   --node-type t3.medium   --nodes 1   --nodes-min 1   --nodes-max 1

Output:
-------

2024-09-18 10:37:32 [ℹ]  eksctl version 0.190.0
2024-09-18 10:37:32 [ℹ]  using region ap-south-1
2024-09-18 10:37:32 [ℹ]  setting availability zones to [ap-south-1a ap-south-1b ap-south-1c]
2024-09-18 10:37:32 [ℹ]  subnets for ap-south-1a - public:192.168.0.0/19 private:192.168.96.0/19
2024-09-18 10:37:32 [ℹ]  subnets for ap-south-1b - public:192.168.32.0/19 private:192.168.128.0/19
2024-09-18 10:37:32 [ℹ]  subnets for ap-south-1c - public:192.168.64.0/19 private:192.168.160.0/19
2024-09-18 10:37:32 [ℹ]  nodegroup "standard-workers" will use "" [AmazonLinux2/1.30]
2024-09-18 10:37:32 [ℹ]  using Kubernetes version 1.30
2024-09-18 10:37:32 [ℹ]  creating EKS cluster "kubernetes-cluster" in "ap-south-1" region with managed nodes
2024-09-18 10:37:32 [ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup
2024-09-18 10:37:32 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=ap-south-1 --cluster=kubernetes-cluster'
2024-09-18 10:37:32 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "kubernetes-cluster" in "ap-south-1"
2024-09-18 10:37:32 [ℹ]  CloudWatch logging will not be enabled for cluster "kubernetes-cluster" in "ap-south-1"
2024-09-18 10:37:32 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=ap-south-1 --cluster=kubernetes-cluster'
2024-09-18 10:37:32 [ℹ]  default addons vpc-cni, kube-proxy, coredns were not specified, will install them as EKS addons
2024-09-18 10:37:32 [ℹ]
2 sequential tasks: { create cluster control plane "kubernetes-cluster",
    2 sequential sub-tasks: {
        2 sequential sub-tasks: {
            1 task: { create addons },
            wait for control plane to become ready,
        },
        create managed nodegroup "standard-workers",
    }
}
2024-09-18 10:37:32 [ℹ]  building cluster stack "eksctl-kubernetes-cluster-cluster"
2024-09-18 10:37:32 [ℹ]  deploying stack "eksctl-kubernetes-cluster-cluster"
2024-09-18 10:38:02 [ℹ]  waiting for CloudFormation stack "eksctl-kubernetes-cluster-cluster"
2024-09-18 10:38:32 [ℹ]  waiting for CloudFormation stack "eksctl-kubernetes-cluster-cluster"
2024-09-18 10:39:32 [ℹ]  waiting for CloudFormation stack "eksctl-kubernetes-cluster-cluster"
2024-09-18 10:40:33 [ℹ]  waiting for CloudFormation stack "eksctl-kubernetes-cluster-cluster"
2024-09-18 10:41:33 [ℹ]  waiting for CloudFormation stack "eksctl-kubernetes-cluster-cluster"
2024-09-18 10:42:33 [ℹ]  waiting for CloudFormation stack "eksctl-kubernetes-cluster-cluster"
2024-09-18 10:43:33 [ℹ]  waiting for CloudFormation stack "eksctl-kubernetes-cluster-cluster"
2024-09-18 10:44:33 [ℹ]  waiting for CloudFormation stack "eksctl-kubernetes-cluster-cluster"
2024-09-18 10:44:33 [!]  recommended policies were found for "vpc-cni" addon, but since OIDC is disabled on the cluster, eksctl cannot configure the requested permissions; the recommended way to provide IAM permissions for "vpc-cni" addon is via pod identity associations; after addon creation is completed, add all recommended policies to the config file, under `addon.PodIdentityAssociations`, and run `eksctl update addon`
2024-09-18 10:44:33 [ℹ]  creating addon
2024-09-18 10:44:34 [ℹ]  successfully created addon
2024-09-18 10:44:34 [ℹ]  creating addon
2024-09-18 10:44:34 [ℹ]  successfully created addon
2024-09-18 10:44:35 [ℹ]  creating addon
2024-09-18 10:44:35 [ℹ]  successfully created addon
2024-09-18 10:46:35 [ℹ]  building managed nodegroup stack "eksctl-kubernetes-cluster-nodegroup-standard-workers"
2024-09-18 10:46:35 [ℹ]  deploying stack "eksctl-kubernetes-cluster-nodegroup-standard-workers"
2024-09-18 10:46:35 [ℹ]  waiting for CloudFormation stack "eksctl-kubernetes-cluster-nodegroup-standard-workers"
2024-09-18 10:47:05 [ℹ]  waiting for CloudFormation stack "eksctl-kubernetes-cluster-nodegroup-standard-workers"
2024-09-18 10:47:51 [ℹ]  waiting for CloudFormation stack "eksctl-kubernetes-cluster-nodegroup-standard-workers"
2024-09-18 10:49:14 [ℹ]  waiting for CloudFormation stack "eksctl-kubernetes-cluster-nodegroup-standard-workers"
2024-09-18 10:49:14 [ℹ]  waiting for the control plane to become ready
2024-09-18 10:49:15 [✔]  saved kubeconfig as "/home/ubuntu/.kube/config"
2024-09-18 10:49:15 [ℹ]  no tasks
2024-09-18 10:49:15 [✔]  all EKS cluster resources for "kubernetes-cluster" have been created
2024-09-18 10:49:15 [✔]  created 0 nodegroup(s) in cluster "kubernetes-cluster"
2024-09-18 10:49:15 [ℹ]  nodegroup "standard-workers" has 1 node(s)
2024-09-18 10:49:15 [ℹ]  node "ip-192-168-11-67.ap-south-1.compute.internal" is ready
2024-09-18 10:49:15 [ℹ]  waiting for at least 1 node(s) to become ready in "standard-workers"
2024-09-18 10:49:15 [ℹ]  nodegroup "standard-workers" has 1 node(s)
2024-09-18 10:49:15 [ℹ]  node "ip-192-168-11-67.ap-south-1.compute.internal" is ready
2024-09-18 10:49:15 [✔]  created 1 managed nodegroup(s) in cluster "kubernetes-cluster"
2024-09-18 10:49:16 [ℹ]  kubectl command should work with "/home/ubuntu/.kube/config", try 'kubectl get nodes'
2024-09-18 10:49:16 [✔]  EKS cluster "kubernetes-cluster" in "ap-south-1" region is ready
ubuntu@gptesting:~/sample$

kubectl apply \
    --validate=false \
    -f https://github.com/jetstack/cert-manager/releases/download/v1.13.5/cert-manager.yaml

kubectl apply -f https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.4.7/v2_4_7_full.yaml

ubuntu@gptesting:~/sample$ aws iam create-policy \
  --policy-name AWSLoadBalancerControllerIAMPolicy \
  --policy-document file://iam_policy.json
{
    "Policy": {
        "PolicyName": "AWSLoadBalancerControllerIAMPolicy",
        "PolicyId": "ANPAZKRGQYHZUFGA4G2ZF",
        "Arn": "arn:aws:iam::641104658931:policy/AWSLoadBalancerControllerIAMPolicy",
        "Path": "/",
        "DefaultVersionId": "v1",
        "AttachmentCount": 0,
        "PermissionsBoundaryUsageCount": 0,
        "IsAttachable": true,
        "CreateDate": "2024-09-18T10:58:51+00:00",
        "UpdateDate": "2024-09-18T10:58:51+00:00"
    }
}
ubuntu@gptesting:~/sample$

eksctl utils associate-iam-oidc-provider \
  --region=ap-south-1 \
  --cluster=kubernetes-cluster \
  --approve

eksctl create iamserviceaccount \
  --cluster kubernetes-cluster \
  --namespace kube-system \
  --name aws-load-balancer-controller \
  --attach-policy-arn arn:aws:iam::641104658931:policy/AWSLoadBalancerControllerIAMPolicy \
  --approve

aws configure set region ap-south-1

aws ec2 create-tags --resources subnet-03636dd3198024e1b --tags Key=kubernetes.io/role/elb,Value=1

kubectl get pods -n kube-system

kubectl get deployment aws-load-balancer-controller -n kube-system

kubectl get serviceaccount aws-load-balancer-controller -n kube-system -o yaml

aws ec2 describe-subnets --query "Subnets[?SubnetId=='subnet-03636dd3198024e1b'].Tags"

kubectl logs deployment/aws-load-balancer-controller -n kube-system

eksctl get nodegroup --cluster kubernetes-cluster

kubectl cluster-info

kubectl get node

Create "alb" class :

# Save as ingressclass-alb.yaml
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: alb
spec:
  controller: ingress.k8s.aws/alb

# deploy
kubectl apply -f ingressclass-alb.yaml

------------------------------------------------------------------------------------------------------------------------------------------------------------

To stop the Node being created itself
=====================================

Autosclaling stop 

aws autoscaling suspend-processes --auto-scaling-group-name eks-standard-workers-1ec9025f-07b6-e06b-0111-51cf0d08f504 --scaling-processes Launch

eks-standard-workers-1ec9025f-07b6-e06b-0111-51cf0d08f504

Verify

aws autoscaling describe-auto-scaling-groups --auto-scaling-group-names eks-standard-workers-1ec9025f-07b6-e06b-0111-51cf0d08f504


Autoscaling resume 

aws autoscaling resume-processes --auto-scaling-group-name eks-standard-workers-1ec9025f-07b6-e06b-0111-51cf0d08f504

Manually Scaling the Node Group: 
Try manually setting the desired capacity to 0, which should stop the node:

aws autoscaling update-auto-scaling-group --auto-scaling-group-name eks-standard-workers-1ec9025f-07b6-e06b-0111-51cf0d08f504 --desired-capacity 0





























